

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="立体角">
  <meta name="keywords" content="">
  
    <meta name="description" content="太久没更博客了，这次先更一个期中作业的报告。本次实验要求实现一个表情识别模型，对五种表情进行分类，并计算分类的准确性。在本次实验中，自己通过数据增强、网络结构优化和模型融合，将模型在测试集上的的识别准确率由开始的57%提高到72%。   image-20240517164431436  一. 数据预处理 本次实验的数据集储存在一个文件夹中，每个文件夹下属5个子文件夹，名字为各个类别的l">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络期中作业-表情识别">
<meta property="og:url" content="http://example.com/2024/05/21/ANN-midterm-homework/index.html">
<meta property="og:site_name" content="Litijiao&#39;s space">
<meta property="og:description" content="太久没更博客了，这次先更一个期中作业的报告。本次实验要求实现一个表情识别模型，对五种表情进行分类，并计算分类的准确性。在本次实验中，自己通过数据增强、网络结构优化和模型融合，将模型在测试集上的的识别准确率由开始的57%提高到72%。   image-20240517164431436  一. 数据预处理 本次实验的数据集储存在一个文件夹中，每个文件夹下属5个子文件夹，名字为各个类别的l">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-05-21T08:37:06.000Z">
<meta property="article:modified_time" content="2024-05-21T08:42:51.639Z">
<meta property="article:author" content="立体角">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>神经网络期中作业-表情识别 - Litijiao&#39;s space</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Litijiao&#39;s space</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/night.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="神经网络期中作业-表情识别"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-05-21 16:37" pubdate>
          2024年5月21日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          31 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">神经网络期中作业-表情识别</h1>
            
            
              <div class="markdown-body">
                
                <p>太久没更博客了，这次先更一个期中作业的报告。本次实验要求实现一个表情识别模型，对五种表情进行分类，并计算分类的准确性。在本次实验中，自己通过数据增强、网络结构优化和模型融合，<strong>将模型在测试集上的的识别准确率由开始的57%提高到72%</strong>。</p>
<figure>
<img
src="C:\Users\15006\AppData\Roaming\Typora\typora-user-images\image-20240517164431436.png" srcset="/img/loading.gif" lazyload
alt="image-20240517164431436" />
<figcaption aria-hidden="true">image-20240517164431436</figcaption>
</figure>
<h3 id="一.-数据预处理">一. 数据预处理</h3>
<p>本次实验的数据集储存在一个文件夹中，每个文件夹下属5个子文件夹，名字为各个类别的label（Angry，Happy等）。这种格式适合于pytroch中的<code>datasets.ImageFolder</code>，这个函数可以将文件夹的所有图片载入进来，同时各个子文件夹的名字作为图片的label：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">train_dataset = datasets.ImageFolder(root=<span class="hljs-string">&#x27;mid_hw_emotion_recognition/train&#x27;</span>)<br>test_dataset = datasets.ImageFolder(root=<span class="hljs-string">&#x27;mid_hw_emotion_recognition/test&#x27;</span>, transform=transform)<br></code></pre></td></tr></table></figure>
<p>对于每张图片，我们需要将其从图片格式转换为张量，对于每张图片，我们将其转换为一个48×48的矩阵，然后将其变为tensor，并对其中的值进行归一化，在归一化中，我们采用(0.485,
0.456, 0.406)和(0.229, 0.224,
0.225)作为归一化的均值和方差，这个值是从ImageNet数据集上通过大量图片数据分析得出的结果，是目前CV领域的一个常见归一化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">48</span>, <span class="hljs-number">48</span>)),  <span class="hljs-comment"># 将图像调整为48x48大小</span><br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], std=[<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])  <span class="hljs-comment"># 采用ImageNet的均值和标准差</span><br>])<br></code></pre></td></tr></table></figure>
<p>然而，此次训练集只有几千张图片，并且各个图片数量并不均匀，为了提高模型的表现，对于训练集采用特别的变换，具体来说，这里增加了<strong>随机裁剪，随机反转，随机旋转</strong>三种数据增强方法。通过数据增强，模型能够提升泛化性，然而数据增强也可能导致模型学习到一些噪声或难以收敛，本次作业测量了5种数据增强方法对于模型性能的提高效果，具体数据见</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">train_transform = transforms.Compose([<br>    transforms.RandomResizedCrop(<span class="hljs-number">48</span>, scale=(<span class="hljs-number">0.8</span>, <span class="hljs-number">1.2</span>)), <span class="hljs-comment">#随机裁剪</span><br>    transforms.RandomHorizontalFlip(), <span class="hljs-comment">#随机翻转</span><br>    transforms.RandomApply([transforms.RandomRotation(<span class="hljs-number">10</span>)], p=<span class="hljs-number">0.5</span>), <span class="hljs-comment">#随机旋转10°，有50%概率发生</span><br>    transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>), (<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>)),<br>]) <br></code></pre></td></tr></table></figure>
<p>接下来需要构建验证集，从训练集取出20%的图片作为验证集，需要注意的是，只有训练集需要应用<code>train_transform</code>，而验证集不需要进行数据增强。因此，这里先不用任何变换随机分割数据集，然后将对应的变换放给训练集和验证集，最后替换原先随机分割的样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">num_train = <span class="hljs-built_in">len</span>(train_dataset) <br>num_val = <span class="hljs-built_in">int</span>(<span class="hljs-number">0.2</span> * num_train) <span class="hljs-comment">#计算验证集的大小</span><br>num_train_actual = num_train - num_val<br><span class="hljs-comment">#随机分割数据集（此时不应用任何变换）</span><br>train_data, val_data = random_split(train_dataset, [num_train_actual, num_val])<br><span class="hljs-comment">#应用不同的变换到训练集和验证集</span><br>train_dataset = datasets.ImageFolder(root=<span class="hljs-string">&#x27;mid_hw_emotion_recognition/train&#x27;</span>, transform=train_transform)<br>val_dataset = datasets.ImageFolder(root=<span class="hljs-string">&#x27;mid_hw_emotion_recognition/train&#x27;</span>, transform=transform)<br><span class="hljs-comment">#替换原来随机分割的数据集中的样本</span><br>train_dataset.samples = train_data.dataset.samples<br>val_dataset.samples = val_data.dataset.samples<br></code></pre></td></tr></table></figure>
<h3 id="二.-神经网络结构">二. 神经网络结构</h3>
<h4 id="一cnn">（一）CNN</h4>
<p>普通的CNN在理论课已经有比较详细的讲解，在这里开始构建了一个两层的CNN网络，初始的<code>in_channel</code>是3（因为图是彩色的），每个卷积层有3个卷积核，因为步长为1，为了保持图像维度不变进行1个padding。池化层采用最大池化。因为经过了两层池化，所以最后每张图片大小是64×12×12（48/2/2），再经过两层全连接层后，得到分类的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CNN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(CNN, self).__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        self.pool = nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        self.fc1 = nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">12</span> * <span class="hljs-number">12</span>, <span class="hljs-number">128</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">5</span>)  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.pool(torch.relu(self.conv1(x)))<br>        x = self.pool(torch.relu(self.conv2(x)))<br>        x = torch.flatten(x, <span class="hljs-number">1</span>)<br>        x = torch.relu(self.fc1(x))<br>        x = self.fc2(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<h4 id="二resnet">（二）Resnet</h4>
<p>对于传统CNN，其层数不断增加后，可能会出现反向传播梯度消失的风险，因此传统CNN层数受到一定的限制，Resnet采用残差连接解决了这个问题，从而可以将层数不断增加。本次作业根据pytorch的Resnet18模型，实现了模型的神经网络结构。对于Resnet，其结构如下所示，图像在经过一层卷积层和池化后进入残差连接层。每个残差连接层有两层卷积层，每个卷积层有64个3×3的卷积核。</p>
<figure>
<img
src="https://pic2.zhimg.com/v2-97e5b4c6d1142e1dffa938215374dac9_b.jpg" srcset="/img/loading.gif" lazyload
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>在每个残差连接层中，其构造如下所示，相较于CNN，Resnet也采用了Batch
Normalization，这也是为了防止数据过拟合。同时，为了保证输入输出的维度保持一致，在计算残差时需要进行一次判定，如果维度不符，那么就需要通过卷积层再做一次提取，这个操作主要见于两个大残差层连接的时候，因为前一层<code>out_channel</code>是第二个大残差层<code>out_channel</code>的1/2，因此需要做一次卷积将其放大维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResBlock</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channel, out_channel, stride=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(ResBlock, self).__init__()<br>        self.conv1 = torch.nn.Conv2d(in_channel, out_channel, kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = torch.nn.BatchNorm2d(out_channel)<br>        self.conv2 = torch.nn.Conv2d(out_channel, out_channel, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn2 = torch.nn.BatchNorm2d(out_channel)<br>        <br>        self.shortcut = torch.nn.Sequential() <span class="hljs-comment"># 如果正常，shortcut就是直接为空，直接+x</span><br>        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> in_channel != out_channel:<br>            self.shortcut = torch.nn.Sequential(<br>                torch.nn.Conv2d(in_channel, out_channel, kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),<br>                torch.nn.BatchNorm2d(out_channel)<br>            )<br>        <span class="hljs-comment"># 这个if是为了防止维度不一致，如果维度不一致需要通过卷积层进行一次提取，以统一维度</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        y = F.relu(self.bn1(self.conv1(x)))<br>        y = self.bn2(self.conv2(y))<br>        y += self.shortcut(x) <span class="hljs-comment"># y除了进行变换，还加上了shortcut(x)</span><br>        <span class="hljs-keyword">return</span> F.relu(y)<br></code></pre></td></tr></table></figure>
<p>接下来，可以构造Resnet的网络架构（这里省略了forward），在该网络中，图像先经过一层卷积层和池化层，然后对于残差层，通过<code>_make_layer</code>来构造残差层。具体来说，这里的大残差层包括四层（<code>layer1</code>到<code>layer4</code>）。每个大残差层有着不同的输出通道，内部可以构造指定层数的小残差层。这样构造的好处在于可以通过改变<code>num_blocks</code>参数来改变网络的架构（例如从Resnet18到Resnet34）。再经过4个大残差层后，就可以通过avgpool进行一次池化，最后输出至全连接层，得到分类的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, block, num_blocks, num_classes=<span class="hljs-number">5</span></span>):<br>        <span class="hljs-built_in">super</span>(ResNet, self).__init__()<br>        self.in_channels = <span class="hljs-number">64</span><br>        self.conv1 = torch.nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = torch.nn.BatchNorm2d(<span class="hljs-number">64</span>)<br>        self.relu = torch.nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.maxpool = torch.nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        self.layer1 = self._make_layer(block, <span class="hljs-number">64</span>, num_blocks[<span class="hljs-number">0</span>], stride=<span class="hljs-number">1</span>)<br>        self.layer2 = self._make_layer(block, <span class="hljs-number">128</span>, num_blocks[<span class="hljs-number">1</span>], stride=<span class="hljs-number">2</span>)<br>        self.layer3 = self._make_layer(block, <span class="hljs-number">256</span>, num_blocks[<span class="hljs-number">2</span>], stride=<span class="hljs-number">2</span>)<br>        self.layer4 = self._make_layer(block, <span class="hljs-number">512</span>, num_blocks[<span class="hljs-number">3</span>], stride=<span class="hljs-number">2</span>)<br>        self.avgpool = torch.nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        self.fc = torch.nn.Linear(<span class="hljs-number">512</span>, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layer</span>(<span class="hljs-params">self, block, out_channels, num_blocks, stride</span>):<br>        <span class="hljs-comment"># 构建指定层数的残差层</span><br>        strides = [stride] + [<span class="hljs-number">1</span>]*(num_blocks-<span class="hljs-number">1</span>)<br>        layers = []<br>        <span class="hljs-keyword">for</span> stride <span class="hljs-keyword">in</span> strides:<br>            layers.append(block(self.in_channels, out_channels, stride))<br>            self.in_channels = out_channels<br>        <span class="hljs-keyword">return</span> torch.nn.Sequential(*layers)<br><br>model = ResNet(ResBlock, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], num_classes=<span class="hljs-number">5</span>).to(device) <span class="hljs-comment"># 残差层2*4*2+卷积层*1+全连接层*1=18</span><br></code></pre></td></tr></table></figure>
<h4 id="三vgg">（三）VGG</h4>
<p>VGG实际上早于Resnet出现，相较于传统CNN，其通过重复使用简单的基础块来构建深度模型，具体来说，<strong>其就是一个大号的CNN</strong>。VGG可以分成几个相似的大层相连接，每个大层包括<strong>一个卷积层、一个BatchNorm层、以及一个激活函数</strong>。对于VGG11网络，其最大维度为512，并在大层中穿插池化层。</p>
<p>由于这几个大层相似，所以同样也可以根据上述的<code>_make_layer</code>进行构造，这里参考了[2]的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VGG</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(VGG, self).__init__()<br>        cfg=[<span class="hljs-number">64</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">128</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>]<br>        self.features = self._make_layers(cfg)<br>        self.classifier = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">5</span>)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layers</span>(<span class="hljs-params">self, cfg</span>):<br>        layers = []<br>        in_channels = <span class="hljs-number">3</span><br>        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> cfg:<br>            <span class="hljs-keyword">if</span> x == <span class="hljs-string">&#x27;M&#x27;</span>:<br>                layers += [nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)]<br>            <span class="hljs-keyword">else</span>:<br>                layers += [nn.Conv2d(in_channels, x, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                           nn.BatchNorm2d(x),<br>                           nn.ReLU(inplace=<span class="hljs-literal">True</span>)]<br>                in_channels = x<br>        layers += [nn.AvgPool2d(kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>)]<br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br></code></pre></td></tr></table></figure>
<h3 id="三.-超参数设置">三. 超参数设置</h3>
<p>本次实验训练涉及以下超参数或优化器设置，在本次实验中，自己选择的设置为：</p>
<ul>
<li>学习率：0.001，这是因为在尝试多个学习率后，发现0.001能够较快的收敛，同时模型表现的性能较好，因此选取该学习率。</li>
<li>损失函数：交叉熵损失，分类常用损失函数，这里有了解Focal
loss的使用方法，但其并不符合本次实验的数据情况，故依旧选取简单有效的交叉熵损失。</li>
<li>优化器：Adam，从经验来看Adam一般结果都不会太差，因此选取该优化器作为此次实验的优化器。</li>
<li>batch_size:64</li>
<li>num_epoch:100</li>
</ul>
<p>Q：是否要选取合适的学习率衰减方法？</p>
<p>A：本次实验尝试了ReduceLROnPlateau这一学习率衰减方法，其根据验证集上的loss更改学习率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="hljs-string">&#x27;min&#x27;</span>, factor=<span class="hljs-number">0.75</span>, patience=<span class="hljs-number">4</span>, verbose=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<p>然而结果显示，这两种学习率衰减方法应用后训练表现并不佳，个人认为这是由于0.001的学习率已经处于比较合适的范围，学习率继续衰减容易导致网络训练时参数更新过慢。并且在一些神经网络的训练上（例如VGG），其训练的时候验证集loss波动较大，因此如果采用ReduceLROnPlateau，很容易导致学习率进行一些不必要的衰减。所以最终自己并没有采用任何学习率衰减方法。为了验证，自己还在Resnet上进行了测试：</p>
<table>
<thead>
<tr>
<th>准确率</th>
<th>None</th>
<th>ReduceLROnPlateau</th>
</tr>
</thead>
<tbody>
<tr>
<td>准确率</td>
<td>64.2%</td>
<td>63.6%</td>
</tr>
</tbody>
</table>
<h3 id="四.-验证结果">四. 验证结果</h3>
<p>对于CNN，其训练图像如下所示，可以看到，训练100个epochs后测试集上准确率为59%。可以看到，传统CNN的效果并不是特别好，训练最后验证集loss训到0.2左右也训不下去了，因此，必须要优化网络结构，采用更为先进的结构才能提高模型的表现。</p>
<figure>
<img
src="C:\Users\15006\AppData\Roaming\Typora\typora-user-images\image-20240517212415719.png" srcset="/img/loading.gif" lazyload
alt="image-20240517212415719" />
<figcaption aria-hidden="true">image-20240517212415719</figcaption>
</figure>
<p>接下来，Resnet18的训练结果如下所示，相较于传统CNN，Resnet18最终在测试集的准确率为64.2%，相较于CNN提高了5.2%，并且最后验证集loss也训到了0.1以下：</p>
<figure>
<img
src="C:\Users\15006\AppData\Roaming\Typora\typora-user-images\image-20240517212626056.png" srcset="/img/loading.gif" lazyload
alt="image-20240517212626056" />
<figcaption aria-hidden="true">image-20240517212626056</figcaption>
</figure>
<p>而VGG11的结果是最好的，相较于Resnet18和CNN，训练了100个epochs最后在测试集上准确率为70.8%！相较于Resnet同样也提升了4.8%：</p>
<figure>
<img
src="C:\Users\15006\AppData\Roaming\Typora\typora-user-images\image-20240517212718720.png" srcset="/img/loading.gif" lazyload
alt="image-20240517212718720" />
<figcaption aria-hidden="true">image-20240517212718720</figcaption>
</figure>
<p>总的来看，从训练结果可以发现，<strong>Happy和Surprise这两个类别普遍较高</strong>，Angry在VGG11的表现上较好，达到了77%的准确度，而<strong>Neutral和Sad的准确率在各个模型上表现均不如其他类别</strong>。个人认为这是因为数据集中不同label的图像具有不同的区分度，对于Happy和Surprise，其表情幅度变化比较大，比较容易区分。但Neutral和Sad这两个类别的图像在一些情况下较难区分，从而降低了准确率。</p>
<h3 id="五.-一些探索和心得体会">五. 一些探索和心得体会</h3>
<h4 id="数据增强">（1）数据增强</h4>
<p>在本次实验中，自己选用了随机裁剪，随机反转，随机旋转三种数据增强手段。数据增强可以对现有的数据集进行扩充，并让模型从多角度去分析图像。然而，不合适的数据增强可能引入误导的噪声，并且让模型难以训练。因此，本次实验自己尝试了五类数据增强方法，分别为随机裁剪，随机反转，随机旋转，随机颜色抖动，随机仿射变换。为了验证他们的有效性，自己采用Resnet在该数据集上训练了40个epoch，分别采用不同的数据增强方法，结果如下所示：</p>
<table>

<thead>
<tr>
<th>方法</th>
<th>Resnet</th>
<th>+随机裁剪</th>
<th>+随机反转</th>
<th>+随机旋转</th>
<th>+随机颜色抖动</th>
<th>+随机仿射变换</th>
</tr>
</thead>
<tbody>
<tr>
<td>准确率</td>
<td>61.2%</td>
<td>63.6%</td>
<td>64.2%</td>
<td>65.4%</td>
<td>59.6%</td>
<td>57.6%</td>
</tr>
</tbody>
</table>
<p>可以发现，随机颜色抖动和随机仿射变换使用后反而差于不使用的结果，而随机旋转对于模型性能提高最大，在测试集上甚至达到了65%的准确率。然而，随机颜色抖动和随机仿射变换的结果反而效果要差于不使用数据增强的结果。因此，自己选取随机反转、随机旋转、随机裁剪三种数据增强方法，作为此次训练集的transform。</p>
<h4 id="模型融合">（2）模型融合</h4>
<p>在训练完Resnet和VGG11模型后，自己思考是否能利用已有模型提升模型在测试集的性能。因此，模型融合成为自己尝试的方向。尽管Resnet在各类label预测准确度都不如VGG，然而，可能在一些图片上Resnet能够更好捕捉数据的特征，从而进行正确的分类。因此，自己对两个模型的输出进行平均，从而得到新的输出结果。由于VGG的表现要好于Resnet，因此自己采用加权平均对两个模型的输出进行融合。具体来说，VGG具有更高的权重（0.8），Resnet的权重则比较低（0.2），在进行加权平均后，得到新的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_ensemble</span>(<span class="hljs-params">model1, model2, <span class="hljs-built_in">input</span>, weight1=<span class="hljs-number">0.8</span>, weight2=<span class="hljs-number">0.2</span></span>):<br>    model1.<span class="hljs-built_in">eval</span>()<br>    model2.<span class="hljs-built_in">eval</span>()<br>    <br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        output1 = model1(<span class="hljs-built_in">input</span>)<br>        output2 = model2(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-comment"># 计算加权平均输出</span><br>        output = output1 * weight1 + output2 * weight2<br>    <br>    <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>
<p>结果显示，模型融合在测试集上展示了更为优越的成果：在模型融合上，模型输出准确率达到了71.8%.这一结果充分说明了模型融合的效果。</p>
<h4 id="模型选择">（3）模型选择</h4>
<p>在本次实验中，传统CNN只达到了59%的准确率，而Resnet和VGG11分别达到了64%和71%的准确率，这主要是因为传统CNN层数太浅，而Resnet18和VGG11通过残差连接和构建更深的网络达到了更好的效果。因此，在构建模型的时候，可以选择一些更为先进的网络结构，从而提高模型的表现效果。</p>
<h3 id="参考资料">参考资料</h3>
<p>[1] https://zhuanlan.zhihu.com/p/451347771</p>
<p>[2] https://zhuanlan.zhihu.com/p/41423739</p>
<p>[3] https://zhuanlan.zhihu.com/p/33101420</p>
<p>[4] ChatGPT</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>神经网络期中作业-表情识别</div>
      <div>http://example.com/2024/05/21/ANN-midterm-homework/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>立体角</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年5月21日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/05/24/crypto-hw8/" title="crypto_hw8">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">crypto_hw8</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/04/09/HCI-2024-04-09/" title="复盘UIST 2024 &amp; 总结HCI一年的旅程">
                        <span class="hidden-mobile">复盘UIST 2024 &amp; 总结HCI一年的旅程</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>

<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js">
</script>Copy
</body>
</html>
